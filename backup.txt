
import os
import gzip
import json
import collections
import math
import numpy
from datasketch import MinHash, MinHashLSH
import warnings
import pandas as pd
import concurrent.futures as cf
import os
import gzip
import json
import collections
import math
import numpy
from datasketch import MinHash, MinHashLSH
import warnings
import pandas as pd
import concurrent.futures as cf

SAMPLE_SIZE = 100

tweet_file = gzip.open("tweets_for_jonah.txt.gz")
loop_stop = SAMPLE_SIZE
tweets = []
for line in tweet_file: 
    loop_stop -= 1
    if loop_stop < 0:
        break
    tweets.append(json.loads(line.decode("utf-8", errors="ignore")))
​

MAX_SEEN_VALUE = .50
MIN_SEEN_VALUE = .01
MATCH_LENIENCY = .3
​
def tweet_to_nameid(tweet):
    return tweet["user"]["name"] + tweet["id_str"]
​
def set_to_minhash(s):
    m = MinHash(num_perm=128)
#     encode_pool = cf.ProcessPoolExecutor(max_workers=8)
#     encodings = [encode_pool.submit((lamda i: i.encode("utf-8")), item) for item in s]
#     for encoding in cf.as_completed(encodings):
#             m.update(encoding)
#     return m
    
    for item in s:
        m.update(item.encode("utf-8"))
    return m
​
def get_words(tweet):
    tweet["words"] = []
    tweet["full_text"] = tweet["full_text"]+"."
    last_ind = 0
    for ind in range(len(tweet["full_text"])):
        if not (tweet["full_text"][ind].isalpha()):
            if last_ind == ind:
                last_ind += 1
                continue
            else:
                tweet["words"].append(tweet["full_text"][last_ind:ind].lower())
                last_ind = ind+1
                
    tweet["word_counts"] = collections.Counter(tweet["words"])
    tweet["minHash"] = set_to_minhash(set(tweet["words"]))
    tweet["nameid"] = tweet_to_nameid(tweet)
    return tweet
​
def determine_usefulness(word, all_words_seen, total_people):
    return ((all_words_seen[word]/total_people > MAX_SEEN_VALUE) or (all_words_seen[word]/total_people < MIN_SEEN_VALUE))
​
def build_people_and_find_words(tweets, all_words_seen):
    total_people = len(tweets)
    return list(filter(lambda word:((all_words_seen[word]/total_people > MAX_SEEN_VALUE) or (all_words_seen[word]/total_people < MIN_SEEN_VALUE)) , all_words_seen.keys()))
​
​
    
# def remove_useless_words(tweets, all_words_seen):
#     total_people = len(tweets)
#     words_to_remove = [word for word in all_words_seen if ((all_words_seen[word]/total_people > MAX_SEEN_VALUE) or (all_words_seen[word]/total_people < MIN_SEEN_VALUE))]
#     for word in words_to_remove:
#         del all_words_seen[word]
#         for tweet in tweets:
#                 if word in tweet["word_counts"]:
#                     del tweet["word_counts"][word]
​
​
# def remove_useless_words(tweets, all_words_seen):
#     total_people = len(tweets)
#     adjustment_pool = cf.ProcessPoolExecutor()
#     word_results = []
#     for word in all_words_seen:
#         word_results.append(adjustment_pool.submit()
​

# cos_dist = sum(ser1*ser2)/(sqrt(sum(ser1^2)) * sqrt(sum(ser2^2)))
def cos_dist(ser1, ser2, tdm, square_sums):
    numerator = tdm[ser1].dot(tdm[ser2])
    ser1_denominator = square_sums[ser1]
    ser2_denominator = square_sums[ser2]
#     if (ser1_denominator*ser2_denominator) == 0:
#         print(ser1, ser2)
    return 1 - numerator/(ser1_denominator*ser2_denominator)
​
def find_sums_for_each_person(tdm):
    square_sums = {person:math.sqrt(sum(map((lambda x: x**2), tdm[person]))) for person in tdm}
    return square_sums
​
# def find_IDFs(tweets):
#     blogs = list(map(set, tweets))
#     blogs = list(map(collections.Counter, blogs))
#     c = collections.Counter()
#     for blog in blogs:
#         c += blog
#     return {word:math.log(BLOG_SAMPLE_SIZE/c[word]) for word in c}

def term_frequency(person):
    return .5 + (.5*person/person.max())
​
def build_tdm(tweets, words_to_remove): #, IDF_Dict
    tdm = pd.DataFrame({tweet["nameid"]:tweet["word_counts"] for tweet in tweets }, dtype=float)
    tdm = tdm.fillna(0)
    tdm = tdm.drop(words_to_remove, axis=0)
    tdm = tdm.drop([col for col in tdm if not (tdm[col]>0).any()], axis=1)
    # (This step scales really badly)
    # IDF_Dict = {}
    # for word in all_words_seen:
    #     personCount = len([person for person in tdm if tdm[person][word] > 0])
    #     if personCount > 0:
    #         IDF_Dict[word] = math.log(BLOG_SAMPLE_SIZE / personCount)
    
​
    # for person in tdm:
    #     tdm[person] = term_frequency(tdm[person])
    #     for i in range(len(person)):
    #         tdm[person][i] = tdm[person][i] * IDF_Dict[tdm[person].keys()[i]]
    return tdm
​
​
# def term_frequency(person):
#     return .5 + (.5*person/person.max())
​
# def build_tdm(tweets, all_words_seen): #, IDF_Dict
#     tdm = pd.DataFrame({tweet["nameid"]:tweet["word_counts"] for tweet in tweets }, dtype=float)
#     tdm = tdm.drop([col for col in tdm if not (tdm[col]>0).any()], axis=1)
#     # (This step scales really badly)
#     # IDF_Dict = {}
#     # for word in all_words_seen:
#     #     personCount = len([person for person in tdm if tdm[person][word] > 0])
#     #     if personCount > 0:
#     #         IDF_Dict[word] = math.log(BLOG_SAMPLE_SIZE / personCount)
    
​
#     # for person in tdm:
#     #     tdm[person] = term_frequency(tdm[person])
#     #     for i in range(len(person)):
#     #         tdm[person][i] = tdm[person][i] * IDF_Dict[tdm[person].keys()[i]]
#     return tdm

def strip_id(user):
    ind = -1
    while not user[ind].isalpha():
        ind -= 1
    return user[:ind+1]
​
def dict_add_person(person, names):
        name = strip_id(person.name)
        node = {"id": person.name, "group":names[name]}
        links = []
        for relation in person.keys():
            if (person[relation] < MATCH_LENIENCY) and (relation != person.name) : 
                links.append({"source":person.name, "target":relation, "value":1})
        return (node, links)
​
def output_to_json(write_path, similarity_frame):
    nodes_and_links = {}
    nodes_and_links["nodes"] = []
    nodes_and_links["links"] = []
    
    with cf.ProcessPoolExecutor(max_workers=8) as executor:
        uniques = set(executor.map(strip_id, similarity_frame.keys()))
    names = dict(zip(uniques, range(len(uniques))))
    json_pool = cf.ProcessPoolExecutor(max_workers=8)
    parsed_results = [json_pool.submit(dict_add_person, similarity_frame[person], names) for person in similarity_frame]
    for node in cf.as_completed(parsed_results):
        nodes_and_links["nodes"].append(node.result()[0])
        nodes_and_links["links"].append(node.result()[1])
    nodes_and_links["links"] = [link for links in nodes_and_links["links"] for link in links]
        
​
    with open(write_path, "w") as file:
        file.write(json.dumps(nodes_and_links, sort_keys=True, indent=2))

with cf.ProcessPoolExecutor() as pool:
    tweets = list(pool.map(get_words, tweets))

prelim_data = list(map(lambda d:(d["nameid"], set_to_minhash(d["word_counts"])), tweets))
prelim_similarities = MinHashLSH(threshold=0.6, num_perm=128)
with prelim_similarities.insertion_session() as session:
    for (key, minhash) in prelim_data:
        session.insert(key, minhash)

pairs_to_check = []
for tweet in tweets:
    pairs = [match for match in prelim_similarities.query(tweet["minHash"]) if match != tweet["nameid"]]
    if len(pairs) > 0:
        pairs_to_check.append((tweet["nameid"], pairs))

word_counts = list(map(lambda d: d["word_counts"], tweets))
packages = []
for i in range(8):
    packages.append(word_counts[((i) * (SAMPLE_SIZE//8)):int((i+1) * (SAMPLE_SIZE//8))])
    
package_pool = cf.ProcessPoolExecutor(max_workers=8)
package_results = [package_pool.submit(sum, counts, collections.Counter()) for counts in packages]
word_sums = [f.result() for f in cf.as_completed(package_results)]

all_words_seen = sum(word_sums, collections.Counter())

words_to_remove = build_people_and_find_words(tweets, all_words_seen)

# IDFs = find_IDFs(blogs)
tdm = build_tdm(tweets, words_to_remove) #build_tdm(people, IDFs, all_words_seen)

square_sums = find_sums_for_each_person(tdm)

processed_tweets = {}
for tweet in tweets:
    processed_tweets[tweet["nameid"]] = False
    
def build_similarities(person, potentials, tdm):
    return (person, {relation:cos_dist(person, relation, tdm, square_sums) for relation in potentials if (relation in tdm and not processed_tweets[relation])})
​
​

distance_pool = cf.ProcessPoolExecutor(max_workers=8)
future_results = []
for (person, potentials) in pairs_to_check:
    if person in tdm:
        processed_tweets[person] = True
        future_results.append(distance_pool.submit(build_similarities, person, potentials, tdm))

similarities = {finished_result.result()[0]:finished_result.result()[1] for finished_result in cf.as_completed(future_results)}
similarity_frame = pd.DataFrame(similarities)

output_to_json("./writeTest.json", similarity_frame)